<h1 id="research-note">Research Note</h1>

<details open="">
  <summary class="text-delta">
    Table of contents
  </summary>
<ul id="markdown-toc">
  <li><a href="#research-note" id="markdown-toc-research-note">Research Note</a>    <ul>
      <li><a href="#feb-2025" id="markdown-toc-feb-2025">Feb. 2025.</a>        <ul>
          <li><a href="#problem-definition" id="markdown-toc-problem-definition">Problem definition</a></li>
          <li><a href="#approach---baseline" id="markdown-toc-approach---baseline">Approach - Baseline</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

</details>

<h2 id="feb-2025">Feb. 2025.</h2>

<h3 id="problem-definition">Problem definition</h3>

<p>Suppose that we’re designing a <code class="language-plaintext highlighter-rouge">single model</code> that solves the ARC-AGI problem. There are several conditions which the model needs to deal with.</p>

<ol>
  <li>The dimensionality of input/output is not pre-determined.</li>
  <li>Though all the tasks can be grouped as a <code class="language-plaintext highlighter-rouge">IQ test</code>, all the detailed tasks have less in common.</li>
  <li>The tasks in evaluation steps are totally different from those in the train steps.</li>
  <li>The model has only 2~9 examples (mostly 3) to solve each task. –&gt;</li>
</ol>

<h3 id="approach---baseline">Approach - Baseline</h3>

<p>Instead of seeking a <code class="language-plaintext highlighter-rouge">direct</code> transformation that converts the input \(X_{in}\) into the output \(X_{out}\), we leverage generative models (such as VAE, flow-based model and diffusion models) to create a latent representation \(X_{latent}\) from both input and output. To achieve this, we introduce two projections:</p>

\[\begin{align}
f: X^{*} \rightarrow X_{in}\\
g: X^{*} \rightarrow X_{out}
\end{align}\]

<p>Here, \(X^*\) represents a higher-dimensional latent space designed to capture complex relationships and interactions between \(X_{in}\) and \(X_{out}\). By utilizing dual parameterized projections \(f\) and \(g\), we enable a more controlled and nuanced learning process, allowing the model to optimize representations that are tailored for both input reconstruction and output generation.</p>
