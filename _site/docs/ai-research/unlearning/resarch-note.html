<h1 id="research-note">Research Note</h1>

<details open="">
  <summary class="text-delta">
    Table of contents
  </summary>
<ul id="markdown-toc">
  <li><a href="#research-note" id="markdown-toc-research-note">Research Note</a>    <ul>
      <li><a href="#tries-to-findout-fundamental-ways-2024" id="markdown-toc-tries-to-findout-fundamental-ways-2024">Tries to findout fundamental ways (2024)</a>        <ul>
          <li><a href="#change-the-ways" id="markdown-toc-change-the-ways">change the ways</a></li>
        </ul>
      </li>
      <li><a href="#srfl-dec-2023" id="markdown-toc-srfl-dec-2023">sRFL (Dec. 2023)</a>        <ul>
          <li><a href="#srfl---introduction" id="markdown-toc-srfl---introduction">sRFL - Introduction</a></li>
          <li><a href="#srfl---pseudo-code" id="markdown-toc-srfl---pseudo-code">sRFL - pseudo code</a></li>
          <li><a href="#srfl---conclusion" id="markdown-toc-srfl---conclusion">sRFL - conclusion</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

</details>

<h2 id="tries-to-findout-fundamental-ways-2024">Tries to findout fundamental ways (2024)</h2>

<h3 id="change-the-ways">change the ways</h3>

<ul>
  <li>Redefine the assessment</li>
  <li>universal framework needed for CV, LM, and so on</li>
  <li>need more mathematical approach</li>
</ul>

<blockquote>
  <p>might we need to interact the data manifold. (2024. 12.)</p>
</blockquote>

<h2 id="srfl-dec-2023">sRFL (Dec. 2023)</h2>

<h3 id="srfl---introduction">sRFL - Introduction</h3>
<p>sRFL is my works based on NeurIPS 2023 Machine Unlearning Challenge’s dataset (celebrities face dataset), It seems that the following methods achieved huge accuracy decoupling between retain and forget dataset in training, seams that it has a chance to achieve the <code class="language-plaintext highlighter-rouge">machine unlearning</code>.</p>

<p>I focused on the decoupling in the logit spaces, using teacher-student framework.</p>

<ul>
  <li>sRFL (simple Rolling in Forget Logits)</li>
</ul>

<h3 id="srfl---pseudo-code">sRFL - pseudo code</h3>
<p>1. <strong>Copy</strong> a model from original trained model</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>teacher_model &lt;- copy.deepcopy(model)
teacher_model.eval() // teacher model will not be trained
</code></pre></div></div>
<p>2. <strong>Freeze layers of model except for first two backbones (feature extracting)</strong></p>

<p>3-1. <strong>Loop : Generate logits by teacher model</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for data in datasets (retain + forget dataset):
    retain_data, forget_data &lt;- data
    pseudo_forget_logits &lt;- teacher_model(forget_data)
    pseudo_retain_logits &lt;- teacher_model(retain_data)
</code></pre></div></div>

<p>3-2. <strong>Roll</strong> only the pseudo_forget_logits   (same mechanism in torch.roll with random roll steps in [-1,1])</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    rolled_pseudo_logits &lt;- roll(pseudo_forget_logits)
</code></pre></div></div>
<p>3-3. <strong>Minimize</strong> KL-divergence between rolled_pseudo_logits and model’s output (from forget data), and use cosine similarity be a regularization and use retain_loss (KL-divergence between pseudo-label(not-rolled) and model outputs about retain dataset)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    forget_outputs &lt;- model(forget_data)
    retain_outputs &lt;- model(retain_data)

    forget_loss &lt;-  KLDiv(rolled_pseudo_logits, forget_outputs) \
                    - cosine_similarity(rolled_pseudo_logits, forget_outputs)\
                    + KLDiv(pseudo_retain_logits, retain_outputs)
</code></pre></div></div>

<p>3-4. <strong>backward</strong> propagation by forget_loss</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   forget_loss.backward()
</code></pre></div></div>
<p>End of Loop.</p>

<h3 id="srfl---conclusion">sRFL - conclusion</h3>

<p>simple Rolling in Forget Logits (sRFL) is a simple way of disturbing the forget label in finetuning framework.</p>

<p>The following is about train accuracy about <code class="language-plaintext highlighter-rouge">CIFAR-10</code></p>

<p align="center">
 <img src="https://sangdo-han.github.io/docs/research/unlearning/cifar10_accuracy.png" />
</p>

<p>In the figure above, facc stands for accuracy of forgetting dataset, racc stands for accuracy of retain dataset.</p>

<p>While it achieved high decoupling, there’s a huge claim about their metrics and evaluation. Even though the competition is completed with some argues, 
the main idea of machine unlearning is quite crucial in privacy and robustness of the ml-based services. In the future, I will progress my works.</p>
