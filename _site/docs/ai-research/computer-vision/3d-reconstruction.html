<h1 id="3d-image-reconstruction">3D Image Reconstruction</h1>

<details open="">
  <summary class="text-delta">
    Table of contents
  </summary>
<ul id="markdown-toc">
  <li><a href="#3d-image-reconstruction" id="markdown-toc-3d-image-reconstruction">3D Image Reconstruction</a>    <ul>
      <li><a href="#3d-reconstruction---introduction" id="markdown-toc-3d-reconstruction---introduction">3D Reconstruction - Introduction</a>        <ul>
          <li><a href="#backgrounds" id="markdown-toc-backgrounds">Backgrounds</a></li>
          <li><a href="#datasets-metrics-and-representations" id="markdown-toc-datasets-metrics-and-representations">Datasets, Metrics and Representations</a>            <ul>
              <li><a href="#datasets" id="markdown-toc-datasets">Datasets</a></li>
              <li><a href="#metrics" id="markdown-toc-metrics">Metrics</a></li>
              <li><a href="#3d-computer-vision-representation" id="markdown-toc-3d-computer-vision-representation">3D Computer Vision Representation</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#references" id="markdown-toc-references">References</a></li>
    </ul>
  </li>
</ul>

</details>

<h2 id="3d-reconstruction---introduction">3D Reconstruction - Introduction</h2>

<p> Introductions to 3D Reconstruction : Backgrounds, Metrics and Representations</p>

<h3 id="backgrounds">Backgrounds</h3>

<p>3D reconstruction, which is 3-Dimensional representation of objects,  can be used for many applications such as video games, animation, navigation and so on.</p>

<p>There are many traditional methods in 3D reconstruction like SfM(Structure from Motion), Dense Reconstruction and MVS(Multi-View Stereo). Those traditional methods are based on photogrammetry.</p>

<p>It is true that understanding the basics of photogrammetry is very important to understand deep-learning based 3D reconstruction because deep-learning based methods are built on top of these photogrammetry techniques.</p>

<p>In this post, however, it is assumed that the readers have knowledge about deep learning models rather than 3D reconstructions. Hence, the discussion will focus on deep-learning based 3D reconstructions while traditional techniques will be mentioned only as needed for readers to comprehend the deep-learning models.</p>

<h3 id="datasets-metrics-and-representations">Datasets, Metrics and Representations</h3>
<p><strong>Based on 3D reconstruction using deep learning : a survey [<a href="#jin-et-al">1</a>]</strong></p>

<p>Starting with a good review paper helps to understand the field and the potential research directions. In deep learning based 3D reconstruction, luckily we can access a good review paper [<a href="#jin-et-al">1</a>] for free.</p>

<h4 id="datasets">Datasets</h4>
<p>The paper[<a href="#jin-et-al">1</a>] show us several useful dataset for 3D reconstructions:</p>

<ol>
  <li>
    <p><a href="https://shapenet.org/">ShapeNet</a>  <br />
It is a very large scale dataset for CAD models developed by Chang et al.[<a href="#shapenet">2</a>] from Stanford University, Princeton University and the Toyota Technological Institute at Chicago, USA.</p>
  </li>
  <li>
    <p><a href="https://cvgl.stanford.edu/projects/pascal3d.html">Pascal3D+</a>  <br />
It is a multi-view datasets, if you are familiar with Object Detection Task, you might be heard of PASCAL VOC dataset. Pascal3D+ has 12 categories of rigid object from PASCAL VOC 2012. This dataset is developed by Yu Xiang et al[<a href="#pascal3d">3</a>] from Computational Vision and Geometry Lab at Stanford University.</p>
  </li>
  <li>
    <p><a href="https://cvgl.stanford.edu/projects/objectnet3d/">ObjectNet3D</a><br />
It is a large scale database for 3D objects. Like as Pascal3D+, this dataset is developed by Yu Xiang et al[<a href="#objectnet">4</a>] from Computational Vision and Geometry Lab at Stanford University.</p>
  </li>
  <li>
    <p><a href="https://www.cvlibs.net/datasets/kitti/">KITTI</a><br />
If you are familiar with Computer Vision in automous driving and mobile robotics, I am sure that you heard of this dataset. This was introduced by Andres Geiger et al[<a href="#kitti-dataset">5</a>].</p>
  </li>
</ol>

<h4 id="metrics">Metrics</h4>
<p>The paper[<a href="#jin-et-ßal">1</a>] briefly explains 3 commonly used metric used: MSE, Voxel IoU and cross-entropy. I think the reader might familiar with MSE and cross-entropy. Therefore, I will skip their details.</p>

<p><strong><em>Voxel IoU</em></strong></p>

<p>In many cases in Computer Vision, IoU represents an abbreviation of Intersection over Union. As you might guess. Voxel IoU is mere volumetric extension of 2D-pixel-IoU. which is:</p>

\[{IoU={G \cap P \over G \cup P}}\]

<p>Here, G stands for a set of voxels in a ground truth, while P stands for a  set of voxels in prediction/reconstruction. This metric is also widely used for 3D object detection or segmentation.</p>

<p>Also, for point cloud and mesh representation, sevaral distance metrics between groundtruth and reconstruction can be used as metric. I beleive that readers are familiar with Euclidean Distance, I will post about Chamfer Distance and EMD instead.</p>

<p><strong><em>Chamfer Distance</em></strong></p>

<p>Chamfer Distance is average of the summation of closest point pairs.</p>

\[Chamfer(G,P) = {1\over{n}}(\sum_i \min_j(||g_i - p_j||) + \sum_j \min_i(||g_i - p_j||))\]

<p>More precisely, The formula represents average of distance from each point in one point cloud to its closest point in the other point cloud. and vice versa.</p>

<p><strong><em>EMD</em></strong></p>

<p>Earth mover’s distance (EMD), also known as the Wasserstein distance, stands for the distance between probability distributions over a region in statistics.</p>

\[EMD(P, \hat{P})=\min_{\phi:P\rightarrow\hat{P}} \sum_{p_i\in P}||p_i - \phi(p_i)||\]

<p>This can be computed using the Hungarian Algorithm or Sinkhorn-Knopp algorithm.</p>

<hr />

<h4 id="3d-computer-vision-representation">3D Computer Vision Representation</h4>

<p>The original survey [<a href="#jin-et-al">1</a>] divides the reconstruction techniques into two main categories by the number of sources. (Reconstruction based on single image or multiple images.) After that, it lists up middle categories with way of representations or how outcome look like with a short descriptions.</p>

<p>These could be very helpful for the researchers who are familiar with 3D Computer Vision. However, for the newcomers (I assume that most of readers of survey papers are newcomers), I think that it would be more effective writing if it explains the ways of representing 3D object first, and then explains the models and results.</p>

<p><strong><em>Voxel Representation</em></strong></p>

<p>Voxel is a volumetric representation which can be comparing to pixel representation in 2D images. Actually, by adding depth (D) into pixel, Voxel achieves representation of 3D.</p>

<p>\(pixel : H \times W \times C\)
\(voxel : H \times W \times D \times C\)
\(, where \ H : height , \ W : width, \ D : depth, \ C : channel \ (color)\)</p>

<p><strong><em>Point Cloud Representation</em></strong></p>

<p>Point Cloud is another way to represent 3D object. A single point cloud is a collection of 3D points (mostly a collection of Cartesian coordinate positions) :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">point_cloud</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">z1</span><span class="p">],</span>
    <span class="p">[</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">z2</span><span class="p">],</span>
    <span class="p">...</span>
    <span class="p">[</span><span class="n">xN</span><span class="p">,</span> <span class="n">yN</span><span class="p">,</span> <span class="n">zN</span><span class="p">]</span>
<span class="p">]</span>
</code></pre></div></div>

<p><strong><em>Mesh Representation</em></strong></p>

<p>Mesh representation could be another option. Mesh is a collection of base geometry to represent surface. Since the least number of points to construct a surface is 3 (triangle), triangle is widely used for mesh representations:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mesh_representation</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="n">point_1</span><span class="p">,</span> <span class="n">point_2</span><span class="p">,</span> <span class="n">point_3</span><span class="p">],</span>
    <span class="p">[</span><span class="n">point_4</span><span class="p">,</span> <span class="n">point_5</span><span class="p">,</span> <span class="n">point_6</span><span class="p">],</span>
    <span class="p">...</span>
    <span class="p">[</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">,</span> <span class="n">point_c</span><span class="p">]</span>
<span class="p">]</span>
</code></pre></div></div>

<p>Each point (vertex) represents 3d point (x,y,z)</p>

<p><strong><em>Other Representation</em></strong></p>

<p><em>Implicit Surface</em></p>

<p>Implicit Surface is a model based surface representation. Models can be a continuous decision boundary of 
deeplearning network classifiers, suggested in OccNet<a href="#OccNet">6</a>, or multi-layer network architecture to extract geometry features 
and represents 3D shapes in an Euclidean preserving latent space as in UCLID-Net<a href="#EUCLID-Net">7</a>
This concept is novel but thinking of the mathematics, we can easily see the concepts:</p>

\[f_{model}(x,y,z) = 0\]

<p>which resembles our well-known implicit surface:</p>

\[f(x,y,z) = x^2+y^2+z^2 -1 =0\]

<p>a sphere!</p>

<p><em>depth</em></p>

<p>About generating depths based on a 2D images, 3D representation also be achieved.</p>

<h2 id="references">References</h2>
<p><span id="jin-et-al">[1]</span> Jin, Y., Jiang, D., &amp; Cai, M. (2020). 3d reconstruction using deep learning: a survey. Communications in Information and Systems, 20(4), 389-413.</p>

<p><span id="shapenet">[2]</span> Chang, A. X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., … &amp; Yu, F. (2015). Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012.</p>

<p><span id="pascal3d">[3]</span> Y. Xiang, R. Mottaghi and S. Savarese, “Beyond PASCAL: A benchmark for 3D object detection in the wild,” IEEE Winter Conference on Applications of Computer Vision, Steamboat Springs, CO, USA, 2014, pp. 75-82, doi: 10.1109/WACV.2014.6836101.</p>

<p><span id="objectnet">[4]</span> Xiang, Y., Kim, W., Chen, W., Ji, J., Choy, C., Su, H., … &amp; Savarese, S. (2016). Objectnet3d: A large scale database for 3d object recognition. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14 (pp. 160-176). Springer International Publishing.</p>

<p><span id="kitti-dataset">[5]</span> A. Geiger, P. Lenz and R. Urtasun, “Are we ready for autonomous driving? The KITTI vision benchmark suite,” 2012 IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, 2012, pp. 3354-3361, doi: 10.1109/CVPR.2012.6248074.</p>

<p><span id="OccNet">[6]</span> L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger,
“Occupancy networks: Learning 3d reconstruction in function space,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2019, pp. 4460–4470.</p>

<p><span id="EUCLID-Net">[7]</span> L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger,
“Occupancy networks: Learning 3d reconstruction in function space,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2019, pp. 4460–4470.</p>
