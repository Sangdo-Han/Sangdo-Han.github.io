<h1 id="llm-utilizations">LLM Utilizations</h1>

<p>Utilizations that ensure robustness in LLM predictions while merely deforming base model architecture.</p>

<details open="">
  <summary class="text-delta">
    Table of contents
  </summary>
<ul id="markdown-toc">
  <li><a href="#llm-utilizations" id="markdown-toc-llm-utilizations">LLM Utilizations</a>    <ul>
      <li><a href="#rag" id="markdown-toc-rag">RAG</a>        <ul>
          <li><a href="#basic-implementation" id="markdown-toc-basic-implementation">Basic Implementation</a></li>
          <li><a href="#output" id="markdown-toc-output">Output</a></li>
        </ul>
      </li>
      <li><a href="#references" id="markdown-toc-references">References</a></li>
    </ul>
  </li>
</ul>

</details>

<h2 id="rag">RAG</h2>

<p>In the adevent of LLM, many AI researchers try to engage pretrained LLM with their own data. This is because training LLM from scratch is costly and hard. 
In this background, one of the powerful engaging method is RAG (Retrieval-augmented generation) [<a href="#lewis-et-al">1</a>]. The concept of RAG is quite simple but powerful. <br />
The following diagram is an architecture of LLM service with RAG, drawn by myself, which could be a little bit different from the original paper[<a href="#lewis-et-al">1</a>].</p>

<pre><code class="language-mermaid">
flowchart LR
    %% Document Indexing Phase
    subgraph Indexing Phase
        A[Document Collection] --&gt; B[Shared Encoder]
        B --&gt; C[Document Embeddings]
        C --&gt; D[Document Index]
    end

    %% Query Processing &amp; Generation Phase
    subgraph Query &amp; Generation Phase
        E[User Query] --&gt; F[Shared Encoder]
        F --&gt; G[Query Embedding]
        G --&gt; H[Similarity Search in Index]
        H --&gt; I[Top-K Relevant Documents]
        I --&gt; J[Augmented Query]
        J --&gt; K[Large Language Model]
        K --&gt; L[Generated Response]
    end


    %% Connect the index from the indexing phase to the search
    D -.-&gt; H

</code></pre>

<p>In batch process, we can save our constarints (contexts, documents or policies) with embedding models, (vectorization / encoding / graph embedding and so on). Once we have a language query from user, we simply encode the query with the same embedding models, and findout the document near the vector. Finally, we simply put found documents and original user’s query to LLM, we can get generated output with our intended contexts.</p>

<h3 id="basic-implementation">Basic Implementation</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">.</span>
├── lib
│   ├── llm
│   └── OpenBLAS
├── scripts
│   └── add_document.py
├── src
│   ├── core.py
│   ├── embedding.py
│   └── utils.py
├── vector_store
│   └── .gitkeep
├── download_model.sh
├── easy_rag.py
├── vector_index.py
└── README.md
</code></pre></div></div>

<p>For detailed installation and usage, please visit my git repository.</p>

<blockquote>
  <p>visit : <a href="https://github.com/Sangdo-Han/miscellaneous/tree/master/llm/rag">easy_rag</a></p>
</blockquote>

<h3 id="output">Output</h3>
<p align="center">
  <img src="https://sangdo-han.github.io/docs/research/llm/rag_chat_example.png" />
</p>

<p>In this example, I put this blog-post into the vector-store. Therefore, as you might notice in the above example, this chat-bot answers depend on this page and the page navigations.</p>
<blockquote>
  <p>Focus on the answer (<code class="language-plaintext highlighter-rouge">The concept of RAG is simple but powerful</code>), which is derived in the first paragraph of this post (<code class="language-plaintext highlighter-rouge">The concept of RAG is quite simple but powerful</code>).</p>
</blockquote>

<h2 id="references">References</h2>
<p><span id="lewis-et-al">[1]</span> Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., … &amp; Kiela, D. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33, 9459-9474.</p>
