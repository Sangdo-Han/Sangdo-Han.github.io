<h1 id="llm">LLM</h1>
<p>Large Language Model studies and utilizations</p>

<h2 id="paradigm-shifts-in-language-models">Paradigm Shifts in Language Models</h2>

<p>From beginning of Markov based approach that predicting next n-words, Language Model (LM) evolved drastically with the Transformers and GPT-2 / BERT - approaches. LLM stands for concurrent (2025) LM, mostly served by big tech companies and startups, most of them are now closed source but several LLM architectures and their way of training are officially open (LLama, Deepseek, and so on.)</p>

<p>(ref. https://arxiv.org/pdf/2303.18223.pdf
)</p>

<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Backbone Architecture</th>
      <th>Description</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>LLM (Large Language Model, early 2020s ~ present)</td>
      <td>Transformers (primarily)</td>
      <td>Scaled up PLMs, dataset size, and computational resources. Trained with Human interactions (RLHF - Reinforcement Learning from Human Feedback) or other Reinforcement Learning techniques</td>
      <td>GPT-3, GPT-4, PaLM-2, Gemini-1.5, LLama-3.1, DeepSeek-V3</td>
    </tr>
    <tr>
      <td>PLM (Pre-trained Language Model, late 2010s)</td>
      <td>Transformers, RNNs (earlier models)</td>
      <td>Massive data induced deep learning model when pre-training, followed by fine-tuning for specific (downstream) tasks. Self-supervised learning with masked language model (MLM - ELMo, BERT, RoBERTa) and causal language model (GPT) applied for training.</td>
      <td>ELMo (RNNs), BERT, RoBERTa, GPT-2 (Transformers)</td>
    </tr>
    <tr>
      <td>NLM (Neural Language Model, early-mid 2010s)</td>
      <td>RNNs, Feedforward Neural Networks</td>
      <td>Introduces neural networks to model language, enabling distributed representations (word vectors) and overcoming some limitations of SLMs.</td>
      <td>word2vec (Feedforward), GloVe (Matrix Factorization), Recurrent Neural Networks</td>
    </tr>
    <tr>
      <td>SLM (Statistical Language Model, Pre 2010s)</td>
      <td>N/A (Statistical methods)</td>
      <td>Traditional probabilistic models (Markov assumption). Predicts next words / context(n-words).</td>
      <td>n-gram language model</td>
    </tr>
  </tbody>
</table>

<p>In brief, there are no huge differences between LLMs and PLMs in model base architecture. However, as a LLM has much larger size in terms of model parameters and it requires much more data, AI scientists tried various training techniques to train / fine-tuning the models.</p>
