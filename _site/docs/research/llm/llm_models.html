<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav ul li a { background-image: none; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="icon" href="/assets/images/favicon.ico" type="image/x-icon"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>sanhan research | Lesson 5</title> <meta name="generator" content="Jekyll v3.10.0" /> <meta property="og:title" content="sanhan research" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Lesson 5" /> <meta property="og:description" content="Lesson 5" /> <link rel="canonical" href="http://localhost:4000/docs/research/llm/llm_models.html" /> <meta property="og:url" content="http://localhost:4000/docs/research/llm/llm_models.html" /> <meta property="og:site_name" content="sanhan research" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="sanhan research" /> <meta name="google-site-verification" content="TumEYHq6xpSNuebLNUkEWUMOUm3w7a6mdrQ33DQPEwA" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Lesson 5","headline":"sanhan research","url":"http://localhost:4000/docs/research/llm/llm_models.html"}</script> <!-- End Jekyll SEO tag --> <!-- Automatically display code inside script tags with type=math/tex using MathJax --> <script type="text/javascript" defer src="/assets/js/mathjax-script-type.js"> </script> <!-- Copied from https://docs.mathjax.org/en/latest/web/components/combined.html --> <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> sanhan research </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in AI Research category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/ai-research" class="nav-list-link">AI Research</a><ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in ARC-AGI category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/ai-research/arc-agi" class="nav-list-link">ARC-AGI</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/ai-research/arc-agi/dataset" class="nav-list-link">ARC-AGI dataset</a></li><li class="nav-list-item"><a href="/docs/ai-research/arc-agi/resarch-note" class="nav-list-link">Research on ARC-AGI</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in LLM category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/ai-research/llm" class="nav-list-link">LLM</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/ai-research/llm/utilizations" class="nav-list-link">LLM Utilizations</a></li><li class="nav-list-item"><a href="/docs/ai-research/llm/control-theory" class="nav-list-link">Control theory of LLM</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Computer Vision category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/ai-research/computer-vision" class="nav-list-link">Computer Vision</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/ai-research/computer-vision/3d-reconstruction" class="nav-list-link">3D Image Reconstruction</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Machine Unlearning category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/ai-research/unlearning" class="nav-list-link">Machine Unlearning</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/ai-research/unlearning/resarch-note" class="nav-list-link">Research on Machine Unlearning</a></li></ul></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Programming category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/programming" class="nav-list-link">Programming</a><ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Algorithms and Data structures category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/programming/algorithms" class="nav-list-link">Algorithms and Data structures</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/programming/algorithms/Useful%20Timer%20Class.html" class="nav-list-link">Useful Timer Class</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Concurrent Programming category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/programming/concurrent-programming" class="nav-list-link">Concurrent Programming</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/programming/concurrent-programming/01.%20Introduction.html" class="nav-list-link">01. Introduction</a></li><li class="nav-list-item"><a href="/docs/programming/concurrent-programming/02.%20Multitasking.html" class="nav-list-link">02. Multitasking</a></li><li class="nav-list-item"><a href="/docs/programming/concurrent-programming/03.%20Multiprocessing.html" class="nav-list-link">03. Multiprocessing</a></li><li class="nav-list-item"><a href="/docs/programming/concurrent-programming/04.%20Multithreading.html" class="nav-list-link">04. Multithreading</a></li><li class="nav-list-item"><a href="/docs/programming/concurrent-programming/05.%20IPC.html" class="nav-list-link">05. Inter-Process Communication (IPC)</a></li><li class="nav-list-item"><a href="/docs/programming/concurrent-programming/06.%20IPC-Message-Passing.html" class="nav-list-link">06. IPC - Message Passing</a></li><li class="nav-list-item"><a href="/docs/programming/concurrent-programming/07.%20IPC-Shared-Memory.html" class="nav-list-link">07. IPC - Shared Memory</a></li><li class="nav-list-item"><a href="/docs/programming/concurrent-programming/08.%20Decomposition.html" class="nav-list-link">08. Decomposition</a></li><li class="nav-list-item"><a href="/docs/programming/concurrent-programming/09.%20Decomposition%20-%20Task.html" class="nav-list-link">09. Decomposition - Task Parallelism</a></li><li class="nav-list-item"><a href="/docs/programming/concurrent-programming/10.%20Decomposition%20-%20Data.html" class="nav-list-link">10. Decomposition - Data Parallelism</a></li><li class="nav-list-item"><a href="/docs/programming/concurrent-programming/11.%20Synchronization.html" class="nav-list-link">11. Synchronization</a></li><li class="nav-list-item"><a href="/docs/programming/concurrent-programming/12.%20Synchronization%20-%20Mutexes%20and%20Semaphores.html" class="nav-list-link">12. Synchronization - Mutexes and Semaphores</a></li><li class="nav-list-item"><a href="/docs/programming/concurrent-programming/13.%20Synchronization%20-%20Deadlocks.html" class="nav-list-link">13. Synchronization - Deadlock</a></li><li class="nav-list-item"><a href="/docs/programming/concurrent-programming/14.%20Synchronization%20-%20Starvation.html" class="nav-list-link">14. Synchronization - Starvation</a></li><li class="nav-list-item"><a href="/docs/programming/concurrent-programming/15.%20Synchronization%20-%20RWLocks%20and%20Spin%20Locks.html" class="nav-list-link">15. Synchronization - RWLock, Spin Lock</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in C / C++ category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/programming/c-cpp" class="nav-list-link">C / C++</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/programming/cpp/01.%20Coding%20Conventions.html" class="nav-list-link">01. Coding Conventions - In Progress</a></li><li class="nav-list-item"><a href="/docs/programming/cpp/02.%20Pointers.html" class="nav-list-link">02. Pointers - In progress</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Python category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/programming/python" class="nav-list-link">Python</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/programming/python/01.%20Typing.html" class="nav-list-link">01. Typing</a></li><li class="nav-list-item"><a href="/docs/programming/python/02.%20Decorator.html" class="nav-list-link">02. Decorators</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Rust category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/programming/rust" class="nav-list-link">Rust</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/programming/rust/introduction" class="nav-list-link">01. Hello, World!</a></li><li class="nav-list-item"><a href="/docs/programming/rust/basic-syntax" class="nav-list-link">02. Basic Syntax in Rust</a></li><li class="nav-list-item"><a href="/docs/programming/rust/memory-related" class="nav-list-link">03. Pointers in Rust</a></li></ul></li></ul></li><li class="nav-list-item"><a href="/docs/mathematics" class="nav-list-link">Mathematics</a></li><li class="nav-list-item"><a href="/docs/devlog" class="nav-list-link">Devlog</a></li><li class="nav-list-item"><a href="/docs/about.html" class="nav-list-link">About</a></li></ul> <ul class="nav-list"><li class="nav-list-item external"> <a href="https://github.com/Sangdo-Han" class="nav-list-link external" > GitHub <svg viewBox="0 0 24 24" aria-labelledby="svg-external-link-title"><use xlink:href="#svg-external-link"></use></svg> </a> </li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search sanhan research" aria-label="Search sanhan research" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <!-- --- layout: default title: Models parent: LLM grand_parent: AI Research math: katex nav_order: 1 permalink: /docs/ai-research/llm/models --- --> <h1 id="llm-models"> <a href="#llm-models" class="anchor-heading" aria-labelledby="llm-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> LLM Models </h1> <details open=""> <summary class="text-delta"> Table of contents </summary> <ul id="markdown-toc"> <li><a href="#llm-models" id="markdown-toc-llm-models">LLM Models</a> <ul> <li><a href="#architectures" id="markdown-toc-architectures">Architectures</a> <ul> <li><a href="#ffn-feed-forward-network" id="markdown-toc-ffn-feed-forward-network">FFN (feed forward network)</a></li> <li><a href="#moe-mixture-of-experts" id="markdown-toc-moe-mixture-of-experts">MoE (Mixture of Experts)</a></li> </ul> </li> <li><a href="#components" id="markdown-toc-components">Components</a> <ul> <li><a href="#multi-latent-transformer" id="markdown-toc-multi-latent-transformer">Multi Latent Transformer</a></li> <li><a href="#multihead-latent-transformer" id="markdown-toc-multihead-latent-transformer">Multihead Latent Transformer</a></li> </ul> </li> <li><a href="#mixture-of-experts-moe" id="markdown-toc-mixture-of-experts-moe">Mixture of Experts (MoE)</a></li> <li><a href="#sparse-attention-models" id="markdown-toc-sparse-attention-models">Sparse Attention Models</a> <ul> <li><a href="#factorized-self-attention" id="markdown-toc-factorized-self-attention">Factorized Self-Attention</a></li> <li><a href="#long-range-adaptations" id="markdown-toc-long-range-adaptations">Long-Range Adaptations</a></li> </ul> </li> <li><a href="#retrieval-augmented-models" id="markdown-toc-retrieval-augmented-models">Retrieval-Augmented Models</a> <ul> <li><a href="#dense-retrieval" id="markdown-toc-dense-retrieval">Dense Retrieval</a></li> <li><a href="#fusion-mechanisms" id="markdown-toc-fusion-mechanisms">Fusion Mechanisms</a></li> </ul> </li> <li><a href="#efficient-training-techniques" id="markdown-toc-efficient-training-techniques">Efficient Training Techniques</a> <ul> <li><a href="#gradient-checkpointing" id="markdown-toc-gradient-checkpointing">Gradient Checkpointing</a></li> <li><a href="#mixed-precision-training" id="markdown-toc-mixed-precision-training">Mixed Precision Training</a></li> </ul> </li> </ul> </li> </ul> </details> <h2 id="architectures"> <a href="#architectures" class="anchor-heading" aria-labelledby="architectures"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Architectures </h2> <h3 id="ffn-feed-forward-network"> <a href="#ffn-feed-forward-network" class="anchor-heading" aria-labelledby="ffn-feed-forward-network"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> FFN (feed forward network) </h3> <h3 id="moe-mixture-of-experts"> <a href="#moe-mixture-of-experts" class="anchor-heading" aria-labelledby="moe-mixture-of-experts"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> MoE (Mixture of Experts) </h3> <h2 id="components"> <a href="#components" class="anchor-heading" aria-labelledby="components"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Components </h2> <h3 id="multi-latent-transformer"> <a href="#multi-latent-transformer" class="anchor-heading" aria-labelledby="multi-latent-transformer"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Multi Latent Transformer </h3> <p>The Transformer architecture, introduced in <a href="https://arxiv.org/abs/1706.03762">“Attention Is All You Need”</a> by Vaswani et al. (2017), revolutionized NLP by replacing recurrence with self-attention. Key components include:</p><pre><code class="language-mermaid">graph LR
    A[Input Embedding] --&gt; B[Positional Encoding]
    B --&gt; C[Encoder Block]
    C --&gt; D[Multi-Head Attention]
    D --&gt; E[Feed-Forward Network]
    E --&gt; F[Layer Normalization]
    F --&gt; G[Encoder Output]
    G --&gt; H[Decoder Block]
    H --&gt; I[Masked Multi-Head Attention]
    I --&gt; J[Cross-Attention Encoder-Decoder]
    J --&gt; K[Feed-Forward Network]
    K --&gt; L[Layer Normalization]
    L --&gt; M[Decoder Output]
</code></pre><ol> <li> <p><strong>Self-Attention Mechanism</strong>:<br /> Computes attention scores between all token pairs using:<br /> \(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)<br /> where ( Q ), ( K ), and ( V ) are query, key, and value matrices.</p> </li> <li> <p><strong>Multi-Head Attention</strong>:<br /> Splits input into multiple heads, applies attention in parallel, and concatenates outputs to capture diverse linguistic patterns.</p> </li> <li> <p><strong>Positional Encoding</strong>:<br /> Adds sinusoidal or learned embeddings to inject positional information into token representations.</p> </li> <li> <p><strong>Encoder-Decoder Architecture</strong>:</p> <ul> <li>The encoder processes input into contextualized representations.</li> <li>The decoder generates outputs autoregressively.</li> <li>Variants like encoder-only (BERT) and decoder-only (GPT) dominate modern LLMs.</li> </ul> </li> </ol><hr /> <h3 id="multihead-latent-transformer"> <a href="#multihead-latent-transformer" class="anchor-heading" aria-labelledby="multihead-latent-transformer"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Multihead Latent Transformer </h3> <p>An extension of the standard Transformer, the <strong>Multihead Latent Transformer</strong> introduces latent spaces within attention heads:</p><pre><code class="language-mermaid">graph TD
    A[Input] --&gt; B[Multihead Latent Projections]
    B --&gt; C[Head 1: Latent Space Z₁]
    B --&gt; D[Head 2: Latent Space Z₂]
    B --&gt; E[Head N: Latent Space Zₙ]
    C --&gt; F[Self-Attention in Z₁]
    D --&gt; G[Self-Attention in Z₂]
    E --&gt; H[Self-Attention in Zₙ]
    F --&gt; I[Concatenate &amp; Project]
    G --&gt; I
    H --&gt; I
    I --&gt; J[Output]
</code></pre><ul> <li><strong>Latent Projections</strong>:<br /> Each head projects inputs into a distinct latent space via learnable transformations, enabling specialized feature extraction.</li> <li><strong>Dynamic Interaction</strong>:<br /> Latent variables allow heads to model complex dependencies (e.g., syntax vs. semantics) while maintaining parallel computation.</li> <li><strong>Applications</strong>:<br /> Effective in multimodal tasks where latent spaces can align text, images, or other modalities.</li> </ul><hr /> <h2 id="mixture-of-experts-moe"> <a href="#mixture-of-experts-moe" class="anchor-heading" aria-labelledby="mixture-of-experts-moe"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Mixture of Experts (MoE) </h2> <p>MoE scales model capacity efficiently by activating subsets of “experts” per input:</p><pre><code class="language-mermaid">graph TD
    A[Input Token] --&gt; B[Gating Network]
    B --&gt; C{Top-k Experts}
    C --&gt; D[Expert 1: FFN]
    C --&gt; E[Expert 2: FFN]
    C --&gt; F[Expert N: FFN]
    D --&gt; G[Weighted Sum]
    E --&gt; G
    F --&gt; G
    G --&gt; H[Output]
</code></pre><ol> <li><strong>Architecture</strong>: <ul> <li><strong>Experts</strong>: Multiple feed-forward networks (FFNs) per layer.</li> <li><strong>Gating Network</strong>: Routes tokens to top-( k ) experts (e.g., ( k=2 )) using softmax probabilities.</li> </ul> </li> <li><strong>Benefits</strong>: <ul> <li><strong>Sparse Activation</strong>: Reduces computation vs. dense models (e.g., Switch Transformer achieves trillion-parameter scale).</li> <li><strong>Specialization</strong>: Experts learn distinct skills (e.g., grammar, facts).</li> </ul> </li> <li><strong>Challenges</strong>: <ul> <li><strong>Load Balancing</strong>: Avoid underused experts via auxiliary loss terms.</li> <li><strong>Training Stability</strong>: Requires careful initialization and gradient clipping.</li> </ul> </li> <li><strong>Examples</strong>: <ul> <li><strong>GShard</strong> (Google): MoE with cross-device expert sharding.</li> <li><strong>Mixtral</strong> (Mistral): Open-source MoE model outperforming dense counterparts.</li> </ul> </li> </ol><hr /> <h2 id="sparse-attention-models"> <a href="#sparse-attention-models" class="anchor-heading" aria-labelledby="sparse-attention-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Sparse Attention Models </h2> <h3 id="factorized-self-attention"> <a href="#factorized-self-attention" class="anchor-heading" aria-labelledby="factorized-self-attention"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Factorized Self-Attention </h3><pre><code class="language-mermaid">graph TD
    A[Input Sequence] --&gt; B[Strided Attention]
    A --&gt; C[Local Attention]
    B --&gt; D[Combine Heads]
    C --&gt; D
    D --&gt; E[Output]
</code></pre><ul> <li><strong>Sparse Transformers</strong> (Child et al., 2019):<br /> Use fixed patterns (e.g., strided or local attention) to reduce computation from ( O(n^2) ) to ( O(n\sqrt{n}) ).</li> <li><strong>Blockwise Attention</strong>: Processes sequences in chunks for memory efficiency.</li> </ul> <h3 id="long-range-adaptations"> <a href="#long-range-adaptations" class="anchor-heading" aria-labelledby="long-range-adaptations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Long-Range Adaptations </h3><pre><code class="language-mermaid">graph TD
    A[Input] --&gt; B[Sliding Window Attention]
    A --&gt; C[Global Attention - CLS Tokens]
    B --&gt; D[Combine]
    C --&gt; D
    D --&gt; E[Output]
</code></pre><ul> <li><strong>Longformer</strong> (Beltagy et al., 2020): Combines local windowed attention with global tokens for document-level tasks.</li> <li><strong>BigBird</strong>: Integrates random, windowed, and global attention to handle sequences up to 8K tokens.</li> </ul><hr /> <h2 id="retrieval-augmented-models"> <a href="#retrieval-augmented-models" class="anchor-heading" aria-labelledby="retrieval-augmented-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Retrieval-Augmented Models </h2> <h3 id="dense-retrieval"> <a href="#dense-retrieval" class="anchor-heading" aria-labelledby="dense-retrieval"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Dense Retrieval </h3> <ul> <li><strong>RETRO</strong> (Borgeaud et al., 2022): Retrieves nearest neighbors from a corpus using dense embeddings, fusing them into decoder layers.<pre><code class="language-mermaid">graph TD
  A[Input Query] --&gt; B[Retrieve Neighbors]
  B --&gt; C[Database: Chunks + Embeddings]
  C --&gt; D[Fusion Layer]
  D --&gt; E[Decoder Cross-Attention]
  E --&gt; F[Generated Output]
</code></pre></li> <li><strong>DPR</strong> (Karpukhin et al., 2020): Dual-encoder architecture for open-domain QA.</li> </ul> <h3 id="fusion-mechanisms"> <a href="#fusion-mechanisms" class="anchor-heading" aria-labelledby="fusion-mechanisms"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Fusion Mechanisms </h3> <ul> <li><strong>Fusion-in-Decoder</strong> (Izacard &amp; Grave, 2021): Concatenates retrieved passages with input for cross-attention in the decoder.</li> </ul><hr /> <h2 id="efficient-training-techniques"> <a href="#efficient-training-techniques" class="anchor-heading" aria-labelledby="efficient-training-techniques"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Efficient Training Techniques </h2> <h3 id="gradient-checkpointing"> <a href="#gradient-checkpointing" class="anchor-heading" aria-labelledby="gradient-checkpointing"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Gradient Checkpointing </h3> <p>Saves memory by recomputing activations during backward passes instead of storing them.</p> <h3 id="mixed-precision-training"> <a href="#mixed-precision-training" class="anchor-heading" aria-labelledby="mixed-precision-training"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Mixed Precision Training </h3> <p>Uses 16-bit floats for faster computation, with master weights in 32-bit for stability.</p><pre><code class="language-mermaid">graph LR
    A[Forward/Backward: FP16] --&gt; B[Weight Update: FP32]
    B --&gt; C[Master Weights: FP32]
</code></pre><hr /> <!-- <details> <summary>Expand for Additional Topics (e.g., Adaptive Computation, Energy-Based Models)</summary> </details> --> <hr> <h2 class="text-delta">Table of contents</h2> <ul> <li> <a href="/docs/programming/cpp/01.%20Coding%20Conventions.html">01. Coding Conventions - In Progress</a> </li> <li> <a href="/docs/programming/cpp/02.%20Pointers.html">02. Pointers - In progress</a> </li> </ul> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> <div class="d-flex mt-2"> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> <script src="https://cdn.jsdelivr.net/npm/mermaid@9.1.6/dist/mermaid.min.js"></script> <script> var config = {} ; mermaid.initialize(config); window.mermaid.init(undefined, document.querySelectorAll('.language-mermaid')); </script> </body> </html>
