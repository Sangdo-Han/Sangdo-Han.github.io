<!-- ---
layout: default
title: Models
parent: LLM
grand_parent: AI Research
math: katex
nav_order: 1
permalink: /docs/ai-research/llm/models
--- -->
<h1 id="llm-models">LLM Models</h1>

<details open="">
  <summary class="text-delta">
    Table of contents
  </summary>
<ul id="markdown-toc">
  <li><a href="#llm-models" id="markdown-toc-llm-models">LLM Models</a>    <ul>
      <li><a href="#architectures" id="markdown-toc-architectures">Architectures</a>        <ul>
          <li><a href="#ffn-feed-forward-network" id="markdown-toc-ffn-feed-forward-network">FFN (feed forward network)</a></li>
          <li><a href="#moe-mixture-of-experts" id="markdown-toc-moe-mixture-of-experts">MoE (Mixture of Experts)</a></li>
        </ul>
      </li>
      <li><a href="#components" id="markdown-toc-components">Components</a>        <ul>
          <li><a href="#multi-latent-transformer" id="markdown-toc-multi-latent-transformer">Multi Latent Transformer</a></li>
          <li><a href="#multihead-latent-transformer" id="markdown-toc-multihead-latent-transformer">Multihead Latent Transformer</a></li>
        </ul>
      </li>
      <li><a href="#mixture-of-experts-moe" id="markdown-toc-mixture-of-experts-moe">Mixture of Experts (MoE)</a></li>
      <li><a href="#sparse-attention-models" id="markdown-toc-sparse-attention-models">Sparse Attention Models</a>        <ul>
          <li><a href="#factorized-self-attention" id="markdown-toc-factorized-self-attention">Factorized Self-Attention</a></li>
          <li><a href="#long-range-adaptations" id="markdown-toc-long-range-adaptations">Long-Range Adaptations</a></li>
        </ul>
      </li>
      <li><a href="#retrieval-augmented-models" id="markdown-toc-retrieval-augmented-models">Retrieval-Augmented Models</a>        <ul>
          <li><a href="#dense-retrieval" id="markdown-toc-dense-retrieval">Dense Retrieval</a></li>
          <li><a href="#fusion-mechanisms" id="markdown-toc-fusion-mechanisms">Fusion Mechanisms</a></li>
        </ul>
      </li>
      <li><a href="#efficient-training-techniques" id="markdown-toc-efficient-training-techniques">Efficient Training Techniques</a>        <ul>
          <li><a href="#gradient-checkpointing" id="markdown-toc-gradient-checkpointing">Gradient Checkpointing</a></li>
          <li><a href="#mixed-precision-training" id="markdown-toc-mixed-precision-training">Mixed Precision Training</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

</details>

<h2 id="architectures">Architectures</h2>
<h3 id="ffn-feed-forward-network">FFN (feed forward network)</h3>

<h3 id="moe-mixture-of-experts">MoE (Mixture of Experts)</h3>

<h2 id="components">Components</h2>
<h3 id="multi-latent-transformer">Multi Latent Transformer</h3>

<p>The Transformer architecture, introduced in <a href="https://arxiv.org/abs/1706.03762">“Attention Is All You Need”</a> by Vaswani et al. (2017), revolutionized NLP by replacing recurrence with self-attention. Key components include:</p>

<pre><code class="language-mermaid">graph LR
    A[Input Embedding] --&gt; B[Positional Encoding]
    B --&gt; C[Encoder Block]
    C --&gt; D[Multi-Head Attention]
    D --&gt; E[Feed-Forward Network]
    E --&gt; F[Layer Normalization]
    F --&gt; G[Encoder Output]
    G --&gt; H[Decoder Block]
    H --&gt; I[Masked Multi-Head Attention]
    I --&gt; J[Cross-Attention Encoder-Decoder]
    J --&gt; K[Feed-Forward Network]
    K --&gt; L[Layer Normalization]
    L --&gt; M[Decoder Output]
</code></pre>

<ol>
  <li>
    <p><strong>Self-Attention Mechanism</strong>:<br />
Computes attention scores between all token pairs using:<br />
\(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)<br />
where ( Q ), ( K ), and ( V ) are query, key, and value matrices.</p>
  </li>
  <li>
    <p><strong>Multi-Head Attention</strong>:<br />
Splits input into multiple heads, applies attention in parallel, and concatenates outputs to capture diverse linguistic patterns.</p>
  </li>
  <li>
    <p><strong>Positional Encoding</strong>:<br />
Adds sinusoidal or learned embeddings to inject positional information into token representations.</p>
  </li>
  <li>
    <p><strong>Encoder-Decoder Architecture</strong>:</p>
    <ul>
      <li>The encoder processes input into contextualized representations.</li>
      <li>The decoder generates outputs autoregressively.</li>
      <li>Variants like encoder-only (BERT) and decoder-only (GPT) dominate modern LLMs.</li>
    </ul>
  </li>
</ol>

<hr />

<h3 id="multihead-latent-transformer">Multihead Latent Transformer</h3>

<p>An extension of the standard Transformer, the <strong>Multihead Latent Transformer</strong> introduces latent spaces within attention heads:</p>

<pre><code class="language-mermaid">graph TD
    A[Input] --&gt; B[Multihead Latent Projections]
    B --&gt; C[Head 1: Latent Space Z₁]
    B --&gt; D[Head 2: Latent Space Z₂]
    B --&gt; E[Head N: Latent Space Zₙ]
    C --&gt; F[Self-Attention in Z₁]
    D --&gt; G[Self-Attention in Z₂]
    E --&gt; H[Self-Attention in Zₙ]
    F --&gt; I[Concatenate &amp; Project]
    G --&gt; I
    H --&gt; I
    I --&gt; J[Output]
</code></pre>

<ul>
  <li><strong>Latent Projections</strong>:<br />
Each head projects inputs into a distinct latent space via learnable transformations, enabling specialized feature extraction.</li>
  <li><strong>Dynamic Interaction</strong>:<br />
Latent variables allow heads to model complex dependencies (e.g., syntax vs. semantics) while maintaining parallel computation.</li>
  <li><strong>Applications</strong>:<br />
Effective in multimodal tasks where latent spaces can align text, images, or other modalities.</li>
</ul>

<hr />

<h2 id="mixture-of-experts-moe">Mixture of Experts (MoE)</h2>

<p>MoE scales model capacity efficiently by activating subsets of “experts” per input:</p>

<pre><code class="language-mermaid">graph TD
    A[Input Token] --&gt; B[Gating Network]
    B --&gt; C{Top-k Experts}
    C --&gt; D[Expert 1: FFN]
    C --&gt; E[Expert 2: FFN]
    C --&gt; F[Expert N: FFN]
    D --&gt; G[Weighted Sum]
    E --&gt; G
    F --&gt; G
    G --&gt; H[Output]
</code></pre>

<ol>
  <li><strong>Architecture</strong>:
    <ul>
      <li><strong>Experts</strong>: Multiple feed-forward networks (FFNs) per layer.</li>
      <li><strong>Gating Network</strong>: Routes tokens to top-( k ) experts (e.g., ( k=2 )) using softmax probabilities.</li>
    </ul>
  </li>
  <li><strong>Benefits</strong>:
    <ul>
      <li><strong>Sparse Activation</strong>: Reduces computation vs. dense models (e.g., Switch Transformer achieves trillion-parameter scale).</li>
      <li><strong>Specialization</strong>: Experts learn distinct skills (e.g., grammar, facts).</li>
    </ul>
  </li>
  <li><strong>Challenges</strong>:
    <ul>
      <li><strong>Load Balancing</strong>: Avoid underused experts via auxiliary loss terms.</li>
      <li><strong>Training Stability</strong>: Requires careful initialization and gradient clipping.</li>
    </ul>
  </li>
  <li><strong>Examples</strong>:
    <ul>
      <li><strong>GShard</strong> (Google): MoE with cross-device expert sharding.</li>
      <li><strong>Mixtral</strong> (Mistral): Open-source MoE model outperforming dense counterparts.</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="sparse-attention-models">Sparse Attention Models</h2>

<h3 id="factorized-self-attention">Factorized Self-Attention</h3>
<pre><code class="language-mermaid">graph TD
    A[Input Sequence] --&gt; B[Strided Attention]
    A --&gt; C[Local Attention]
    B --&gt; D[Combine Heads]
    C --&gt; D
    D --&gt; E[Output]
</code></pre>
<ul>
  <li><strong>Sparse Transformers</strong> (Child et al., 2019):<br />
Use fixed patterns (e.g., strided or local attention) to reduce computation from ( O(n^2) ) to ( O(n\sqrt{n}) ).</li>
  <li><strong>Blockwise Attention</strong>: Processes sequences in chunks for memory efficiency.</li>
</ul>

<h3 id="long-range-adaptations">Long-Range Adaptations</h3>

<pre><code class="language-mermaid">graph TD
    A[Input] --&gt; B[Sliding Window Attention]
    A --&gt; C[Global Attention - CLS Tokens]
    B --&gt; D[Combine]
    C --&gt; D
    D --&gt; E[Output]
</code></pre>
<ul>
  <li><strong>Longformer</strong> (Beltagy et al., 2020): Combines local windowed attention with global tokens for document-level tasks.</li>
  <li><strong>BigBird</strong>: Integrates random, windowed, and global attention to handle sequences up to 8K tokens.</li>
</ul>

<hr />

<h2 id="retrieval-augmented-models">Retrieval-Augmented Models</h2>

<h3 id="dense-retrieval">Dense Retrieval</h3>

<ul>
  <li><strong>RETRO</strong> (Borgeaud et al., 2022): Retrieves nearest neighbors from a corpus using dense embeddings, fusing them into decoder layers.
    <pre><code class="language-mermaid">graph TD
  A[Input Query] --&gt; B[Retrieve Neighbors]
  B --&gt; C[Database: Chunks + Embeddings]
  C --&gt; D[Fusion Layer]
  D --&gt; E[Decoder Cross-Attention]
  E --&gt; F[Generated Output]
</code></pre>
  </li>
  <li><strong>DPR</strong> (Karpukhin et al., 2020): Dual-encoder architecture for open-domain QA.</li>
</ul>

<h3 id="fusion-mechanisms">Fusion Mechanisms</h3>

<ul>
  <li><strong>Fusion-in-Decoder</strong> (Izacard &amp; Grave, 2021): Concatenates retrieved passages with input for cross-attention in the decoder.</li>
</ul>

<hr />

<h2 id="efficient-training-techniques">Efficient Training Techniques</h2>

<h3 id="gradient-checkpointing">Gradient Checkpointing</h3>

<p>Saves memory by recomputing activations during backward passes instead of storing them.</p>

<h3 id="mixed-precision-training">Mixed Precision Training</h3>

<p>Uses 16-bit floats for faster computation, with master weights in 32-bit for stability.</p>
<pre><code class="language-mermaid">graph LR
    A[Forward/Backward: FP16] --&gt; B[Weight Update: FP32]
    B --&gt; C[Master Weights: FP32]
</code></pre>
<hr />

<!-- <details>
<summary>Expand for Additional Topics (e.g., Adaptive Computation, Energy-Based Models)</summary>
</details> -->

